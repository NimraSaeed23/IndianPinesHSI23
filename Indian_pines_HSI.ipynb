{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**GoogleNetModel **"
      ],
      "metadata": {
        "id": "pmhSVHb-maKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxqk8dM1qjAF",
        "outputId": "73a41708-35ee-450c-fd5b-f6342ecf7a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spectral in /usr/local/lib/python3.10/dist-packages (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n",
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.23.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install spectral\n",
        "!pip install vit-keras\n",
        "!pip install tensorflow-addons\n",
        "# Loading Drive for Colab\n",
        "from google.colab import drive\n",
        "import urllib\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import seaborn as sns\n",
        "import spectral\n",
        "import spectral.io.envi as envi\n",
        "from sklearn.decomposition import (IncrementalPCA, KernelPCA, PCA, SparsePCA,\n",
        "                                   TruncatedSVD)\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                             cohen_kappa_score, confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import (Conv3D,Conv2D, Dense, Dropout, Flatten, Input,\n",
        "                          Reshape,MaxPooling2D)\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "# from keras.utils import np_utils\n",
        "from tensorflow.keras.layers import (Activation, Lambda, multiply)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import legacy\n",
        "from operator import truediv\n",
        "from vit_keras import utils, vit\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Colab Drive for Loading Datasets\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV95vT2yqqQz",
        "outputId": "31957401-d04b-4138-a086-cd44ff140248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading Hyperspectral Datasets\n",
        "def LoadHSIData(method):\n",
        "    HSI_url = ['http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat',\n",
        "               'http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat']\n",
        "    for i in range(2):\n",
        "        file_name = HSI_url[i].split('/')[-1]\n",
        "        data_path = os.path.join(os.getcwd(),'/content/drive/My Drive/'+str(file_name))\n",
        "        if os.path.exists(data_path) == False:\n",
        "            print(\"Downloading data file from %s to %s\" % (HSI_url[i], data_path))\n",
        "            urllib.request.urlretrieve(url=HSI_url[i], filename=data_path)\n",
        "            print(str(file_name)+\" is Successfully downloaded\")\n",
        "        else:\n",
        "            print(str(file_name) + \" already exists\")\n",
        "    print('All HSI dataset exist!')\n",
        "    ## Loading Datasets\n",
        "    data_path = os.path.join(os.getcwd(),'/content/drive/My Drive/')\n",
        "    if method == 'IP':\n",
        "        HSI = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
        "        GT = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "        Num_Classes = 16\n",
        "    return HSI, GT, Num_Classes"
      ],
      "metadata": {
        "id": "fHkRe7KoqtDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Different Dimensional Reduction Methods\n",
        "def DLMethod(method, HSI, NC = 75):\n",
        "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
        "    if method == 'PCA': ## PCA\n",
        "        pca = PCA(n_components = NC, whiten = True)\n",
        "        RHSI = pca.fit_transform(RHSI)\n",
        "        RHSI = np.reshape(RHSI, (HSI.shape[0], HSI.shape[1], NC))\n",
        "    return RHSI"
      ],
      "metadata": {
        "id": "9rdJJ0-iq6Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrTeSplit(HSI, GT, trRatio, vrRatio, teRatio, randomState=345):\n",
        "    # Split into train and test sets\n",
        "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=teRatio,\n",
        "                                        random_state=randomState, stratify=GT)\n",
        "    # Calculate the validation ratio based on the updated test and train ratios\n",
        "    totalTrRatio = trRatio + vrRatio\n",
        "    new_vrRatio = vrRatio / totalTrRatio\n",
        "    # Split train set into train and validation sets\n",
        "    Tr, Va, TrC, VaC = train_test_split(Tr, TrC, test_size=new_vrRatio,\n",
        "                                        random_state=randomState, stratify=TrC)\n",
        "\n",
        "    return Tr, Va, Te, TrC, VaC, TeC"
      ],
      "metadata": {
        "id": "MYU-zo9sq8nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Global Parameters for Loop\n",
        "HSID = \"IP\"\n",
        "DLM = \"PCA\"\n",
        "WS = 9      ## 3, 5, 7, 9, 11, 13, 15\n",
        "teRatio = 0.20\n",
        "vrRatio = 0.50\n",
        "trRatio = 0.50\n",
        "k = 15\n",
        "adam = tf.keras.optimizers.legacy.Adam(lr = 0.0001, decay = 1e-06)\n",
        "epochs = 50\n",
        "batch_size = 56"
      ],
      "metadata": {
        "id": "RAbUnG-rq-hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3-D HSI slices\n",
        "def ImageCubes(HSI, GT, WS = WS, removeZeroLabels = True):\n",
        "    margin = int((WS - 1) / 2)\n",
        "    zeroPaddedX = ZeroPad(HSI, margin = margin)\n",
        "    ## split patches\n",
        "    patchesData = np.zeros((HSI.shape[0] * HSI.shape[1], WS, WS, HSI.shape[2]))\n",
        "    patchesLabels = np.zeros((HSI.shape[0] * HSI.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = GT[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "## Padding around HSI\n",
        "def ZeroPad(HSI, margin = 2):\n",
        "    NHSI = np.zeros((HSI.shape[0] + 2 * margin, HSI.shape[1] + 2* margin, HSI.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    NHSI[x_offset:HSI.shape[0] + x_offset, y_offset:HSI.shape[1] + y_offset, :] = HSI\n",
        "    return NHSI\n",
        "## Compute the Patch to Prepare for Ground Truths\n",
        "def Patch(HSI,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+WS)\n",
        "    width_slice = slice(width_index, width_index+WS)\n",
        "    patch = HSI[height_slice, width_slice, :]\n",
        "    return patch"
      ],
      "metadata": {
        "id": "TLeVYCiSrCGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Assigning Class Labels for Final Classification and Confusion Matrices\n",
        "def ClassificationReports(TeC, HSID, Te_Pred):\n",
        "    Te_Pred = np.argmax(Te_Pred, axis=1)\n",
        "    if HSID == 'IP':\n",
        "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-mowed',\n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings',\n",
        "                        'Stone-Steel']\n",
        "    elif HSID == 'SA':\n",
        "        target_names = ['Weeds_1','Weeds_2','Fallow',\n",
        "                        'Fallow_rough_plow','Fallow_smooth', 'Stubble','Celery',\n",
        "                        'Grapes_untrained','Soil_vinyard_develop','Corn_Weeds',\n",
        "                        'Lettuce_4wk','Lettuce_5wk','Lettuce_6wk',\n",
        "                        'Lettuce_7wk', 'Vinyard_untrained','Vinyard_trellis']\n",
        "    elif HSID == 'PU':\n",
        "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted','Soil','Bitumen',\n",
        "                        'Bricks','Shadows']\n",
        "    elif HSID == 'KSC':\n",
        "        target_names = ['Scrub', 'Willow Swamp', 'CP/Oak', 'CP hammock', 'Slash Pine', 'Oak/Broadleaf', 'Hardwood Swamp',\n",
        "                        'Graminoid Marsh', 'Spartina Marsh', 'Cattail Marsh', 'Salt Marsh', 'Mud Flats', 'Water']\n",
        "    elif HSID == 'BS':\n",
        "        target_names = ['Water', 'Hippo Grass', 'Floodplain Grasses 1', 'Floodplain Grasses 2',\n",
        "                        'Reeds 1', 'Riparian', 'Firescar 2', 'Island Interior', 'Woodlands',\n",
        "                        'Acacia Shrublands', 'Acacia Grasslands', 'Short Mopane', 'Mixed Mopane', 'Exposed Soils']\n",
        "    elif HSID == 'PC':\n",
        "        target_names = ['Water', 'Trees', 'Asphalt', 'Bricks', 'Bitumen', 'Tiles', 'Shadows',\n",
        "                        'Meadows', 'Soil']\n",
        "    elif HSID == 'SLA':\n",
        "        target_names = ['Brocoli 1', 'Corn weeds', 'Lettuce 4wk', 'Lettuce 5wk',\n",
        "                       'Lettuce 6wk', 'Lettuce 7wk']\n",
        "    elif HSID == 'PC':\n",
        "        target_names = ['Water', 'Trees', 'Asphalt', 'Bricks', 'Bitumen', 'Tiles', 'Shadows',\n",
        "                        'Meadows', 'Soil']\n",
        "    elif HSID == 'UH':\n",
        "        target_names = ['Healthy grass', 'Stressed grass', 'Synthetic grass', 'Trees',\n",
        "                    'Soil', 'Water', 'Residential', 'Commercial', 'Road',\n",
        "                    'Highway', 'Railway', 'Parking Lot 1', 'Parking Lot 2',\n",
        "                    'Tennis Court', 'Running Track']\n",
        "    classification = classification_report(np.argmax(TeC, axis=1), Te_Pred, target_names = target_names)\n",
        "    oa = accuracy_score(np.argmax(TeC, axis=1), Te_Pred)\n",
        "    confusion = confusion_matrix(np.argmax(TeC, axis=1), Te_Pred)\n",
        "    list_diag = np.diag(confusion)\n",
        "    list_raw_sum = np.sum(confusion, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    aa = np.mean(each_acc)\n",
        "    kappa = cohen_kappa_score(np.argmax(TeC, axis=1), Te_Pred)\n",
        "    return classification, confusion, oa*100, each_acc*100, aa*100, kappa*100, target_names\n",
        "\n",
        "## Writing Results in CSV files\n",
        "def CSVResults(file_name, classification, confusion, Tr_Time, Te_Time, DL_Time, kappa, oa, aa, each_acc):\n",
        "    classification = str(classification)\n",
        "    confusion = str(confusion)\n",
        "    with open(file_name, 'w') as CSV_file:\n",
        "      CSV_file.write('{} Tr_Time'.format(Tr_Time))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{} Te_Time'.format(Te_Time))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{} DL_Time'.format(DL_Time))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{} Overall accuracy (%)'.format(oa))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{} Average accuracy (%)'.format(aa))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{}'.format(classification))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{}'.format(each_acc))\n",
        "      CSV_file.write('\\n')\n",
        "      CSV_file.write('{}'.format(confusion))\n",
        "    return CSV_file\n",
        "\n",
        "## Plot and Save Confusion Matrix\n",
        "def Conf_Mat(Te_Pred, TeC, target_names):\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "    Te_Pred = np.argmax(Te_Pred, axis=1)\n",
        "    confusion = confusion_matrix(np.argmax(TeC, axis=1), Te_Pred, labels=np.unique(np.argmax(TeC, axis=1)))\n",
        "    cm_sum = np.sum(confusion, axis=1, keepdims=True)\n",
        "    cm_perc = confusion / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(confusion).astype(str)\n",
        "    nrows, ncols = confusion.shape\n",
        "    for l in range(nrows):\n",
        "      for m in range(ncols):\n",
        "        c = confusion[l, m]\n",
        "        p = cm_perc[l, m]\n",
        "        if l == m:\n",
        "          s = cm_sum[l]\n",
        "          annot[l, m] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "          annot[l, m] = ''\n",
        "        else:\n",
        "          annot[l, m] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(confusion, index=np.unique(target_names), columns=np.unique(target_names))\n",
        "    return cm, annot\n",
        "\n",
        "## Plot Ground Truths\n",
        "def GT_Plot(RDHSI, GT, model, WS, k):\n",
        "  RDHSI = RDHSI.reshape(-1, WS, WS, k, 1)\n",
        "  Predicted = model.predict(RDHSI)\n",
        "  Predicted = np.argmax(Predicted, axis=1)\n",
        "  height, width = np.shape(GT)\n",
        "  ## Calculate the predicted Ground Truths\n",
        "  outputs = np.zeros((height, width))\n",
        "  count = 0\n",
        "  for AA in range(height):\n",
        "    for BB in range(width):\n",
        "      target = int(GT[AA,BB])\n",
        "      if target == 0:\n",
        "        continue\n",
        "      else:\n",
        "        outputs[AA][BB] = Predicted[count]\n",
        "        count = count+1\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "QQr2PaAprF_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Main Function to load Datasets, Dimensional Reduction and Creating Patchs for CNN\n",
        "HSI, GT, Num_Classes = LoadHSIData(HSID)\n",
        "## Reduce the Dimensionality\n",
        "#if k < HSI.shape[2]:\n",
        "start = time.time()\n",
        "RDHSI = DLMethod(DLM, HSI, NC = k)\n",
        "end = time.time()\n",
        "DL_Time = end - start\n",
        "## Create Image Cubes for Model Building\n",
        "CRDHSI, CGT = ImageCubes(RDHSI, GT, WS = WS)\n",
        "## Split Train/validation and Test sets\n",
        "Tr, Va, Te, TrC, VaC, TeC = TrTeSplit(CRDHSI, CGT, trRatio, vrRatio, teRatio)\n",
        "# Reshape train, validation, and test sets\n",
        "Tr = Tr.reshape(-1, WS, WS, k, 1)\n",
        "TrC = to_categorical(TrC)\n",
        "Va = Va.reshape(-1, WS, WS, k, 1)\n",
        "VaC = to_categorical(VaC)\n",
        "Te = Te.reshape(-1, WS, WS, k, 1)\n",
        "TeC = to_categorical(TeC)"
      ],
      "metadata": {
        "id": "8mCA3qWorJKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd69341c-fc40-40cd-bbec-0186fe539028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian_pines_corrected.mat already exists\n",
            "Indian_pines_gt.mat already exists\n",
            "All HSI dataset exist!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GoogleNetModel(WS, k, Num_Classes):\n",
        "    input_shape = (WS, WS, k)\n",
        "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(Num_Classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model"
      ],
      "metadata": {
        "id": "aPyKcEAY909u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GoogleNetModel(input_shape, num_classes):\n",
        "    # Define your custom GoogleNet-like model architecture here\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # Example: stack convolutional and pooling layers\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_tensor)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "    # Add more layers as per GoogleNet architecture\n",
        "\n",
        "    # Flatten and add Dense layers for classification\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "0pMw8bM8_6uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model_name, Tr, TrC, Va, VaC, Te, TeC, adam, CRDHSI, HSID, teRatio, k, WS, DLM, RDHSI, GT,Num_Classes,batch_size,epochs):\n",
        "\n",
        "    # Calling Custom Model\n",
        "    model = model_name(WS, k, Num_Classes)\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    start = time.time()\n",
        "    history = model.fit(x=Tr, y=TrC, batch_size=batch_size, epochs=epochs, validation_data=(Va, VaC))\n",
        "    end = time.time()\n",
        "    Tr_Time = end - start\n",
        "\n",
        "    # Predicting with the model\n",
        "    start = time.time()\n",
        "    Te_Pred = model.predict(Te)\n",
        "    end = time.time()\n",
        "    Te_Time = end - start\n",
        "\n",
        "    # Classification Report\n",
        "    classification,confusion,oa,each_acc,aa,kappa,target_names = ClassificationReports(TeC, HSID, Te_Pred)\n",
        "    print(classification)\n",
        "    # file_name = f\"str(HSID)+str(teRatio)+str(WS)+str(DLM)+str(k)+Report_{model_name.__name__}.csv\"\n",
        "    file_name = f\"{HSID}_{teRatio}_{k}_{WS}_{DLM}_Report_{model_name.__name__}.csv\"\n",
        "    CSV_file = CSVResults(file_name, classification, confusion, Tr_Time, Te_Time, DL_Time, kappa, oa, aa, each_acc)\n",
        "    files.download(file_name)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm, annot = Conf_Mat(Te_Pred, TeC, target_names)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots(figsize=(15,15))\n",
        "    sns.heatmap(cm, cmap= \"Spectral\", annot=annot, fmt='', ax=ax, linewidths=0.5)\n",
        "    file_name = f\"{HSID}_{teRatio}_{k}_{WS}_{DLM}_Confusion_Matrix_{model_name.__name__}.png\"\n",
        "    plt.savefig(file_name, dpi=500)\n",
        "    files.download(file_name)\n",
        "\n",
        "    # Ground Truths\n",
        "    outputs = GT_Plot(CRDHSI, GT, model, WS, k)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(outputs, cmap='nipy_spectral')\n",
        "    plt.axis('off')\n",
        "    file_name = f\"{HSID}_{teRatio}_{k}_{WS}_{DLM}_Ground_Truths_{model_name.__name__}.png\"\n",
        "    plt.savefig(file_name, dpi=500)\n",
        "    files.download(file_name)\n",
        "    return history"
      ],
      "metadata": {
        "id": "gaVDwbsQrf-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model_name, Tr, TrC, Va, VaC, Te, TeC, adam, CRDHSI, HSID, teRatio, k, WS, DLM, RDHSI, GT, Num_Classes, batch_size, epochs):\n",
        "    # Calling Custom Model with input_shape and num_classes\n",
        "    model = model_name((WS, WS, k), Num_Classes)\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "A9TfR3JwHW0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [GoogleNetModel]  # Add your model names here\n",
        "\n",
        "# Loop over the model names and run train_and_evaluate_model() for each model\n",
        "history_list = []\n",
        "for model_name in model_names:\n",
        "    history = train_and_evaluate_model(\n",
        "        model_name, Tr, TrC, Va, VaC, Te, TeC,\n",
        "        adam, CRDHSI, HSID, teRatio, k, WS, DLM, RDHSI, GT,\n",
        "        Num_Classes, batch_size, epochs  # Add epochs here\n",
        "    )\n",
        "    history_list.append(history)\n",
        "\n"
      ],
      "metadata": {
        "id": "54H_VgNQrmya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52106eb-d875-490a-ac9d-25d712a7ffc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 9, 9, 15)]        0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 9, 9, 64)          8704      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 4, 4, 64)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                16400     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1074704 (4.10 MB)\n",
            "Trainable params: 1074704 (4.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}